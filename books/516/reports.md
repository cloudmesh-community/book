# Report List

Here we summarize the Projects and teams. Place the document in your
github folder under project/report.md. Please make sure to use proper
markdown and not githubs common mark. Once the reports are created they
will at one point also show up in the proceedings.

## CLOUD STORAGE-STORAGE-LOCAL PROJECTS

| HID	| Status |	Names |	Project| 
|-|--|---|---------------|
| [160] | approved |	Shreyans |	[Azure to Google Cloudmesh Storage Provider for Virtual Directories](https://github.com/cloudmesh-community/fa19-516-160/blob/master/project/report.md): see cloudmesh-storage to start. develop OpenAPI REST services for it|
| [152] | approved | Pratibha	| [Google to/form AWS: Cloudmesh Storage Provider for Virtual Directories](https://github.com/cloudmesh-community/fa19-516-152/blob/master/project/report.md): see cloudmesh-storage to start. develop OpenAPI REST services for it| 
| [162]	| approved | Shivani | [Oracle to/from ? Storage Service](https://github.com/cloudmesh-community/fa19-516-162/blob/master/project/report.md) (Residential) 534, (see: cloudmesh-storage), Provide a REST service for it based on OpenAPI that uses the cloudmesh API you develop. see also Python, implement virtual directory from local first |
| [155]	| approved | Ketan	| [Azure to/form AWS blob Storage Service](https://github.com/cloudmesh-community/fa19-516-155/blob/master/project/report.md), (see: cloudmesh-storage), Provide a REST service for it based on OpenAPI that uses the cloudmesh API you develop. |
| all   |	       | |  if this works technically we should be able to coppy between all clouds as we have all services for each cloud and each speaks the same protocol |

[160]: https://github.com/cloudmesh-community/fa19-516-160
[152]: https://github.com/cloudmesh-community/fa19-516-152
[162]: https://github.com/cloudmesh-community/fa19-516-162
[155]: https://github.com/cloudmesh-community/fa19-516-155


## CLOUDMESH COMPUTE PROJECTS

| HID	| Status |	Names |	Project| 
|-|--|---|---------------|
| [162](https://github.com/cloudmesh-community/fa19-516-162/blob/master/project/report.md)	| approved	| Shivani | 	516: Oracle Compute Service (Residential), see also Python |
| [165](https://github.com/cloudmesh-community/fa19-516-165/blob/master/project/report.md)	| approved	| Zhi	| Google Compute Service, gregor will set up new repo for this, you can start identifying python API and implement functions to list images, flavors, start vms upload keys and security groups, once repo is set up you need to start uploading your contributions on weekly basis |
| [170](https://github.com/cloudmesh-community/fa19-516-170/blob/master/project/report.md)	| approved	| Chenxu |	Azure Cloudmesh Compute - The secgroup command is not working, extensive testing needs to be done. As this project is mostly finished, you will compare the performance from Azure with other clouds such as OpenStack, AWS, and possibly Oracle, and Google once they become available. You would help improve any of the missing clouds once secgroup is done (Oracle, Google). IT is best to start right away with this and learn how to start vms in Azure. |
| [169](https://github.com/cloudmesh-community/fa19-516-169/blob/master/project/report.md)	| approved	| Harsh	| Developing Cloudmesh interfaces for Google Cloud Platform. The two cloud implementation to be demonstrated using GCP and AWS |

## PI PROJECTS

| HID	| Status |	Names |	Project| 
|-|--|---|---------------|
| [148](https://github.com/cloudmesh-community/fa19-516-148/blob/master/project/report.md)	| approved |	Sub|	Federated Kubernetes Cluster with Raspberry PI |
| [158](https://github.com/cloudmesh-community/fa19-516-158/blob/master/project/report.md) [150](https://github.com/cloudmesh-community/fa19-516-150/blob/master/project/report.md)	| approved	| Daivik Akshay	| Raspberry Pi Cloud Cluster for Spark, Raspberry Pi Cloud Cluster for Hadoop |

## DATABASE ABSTRACTION PROJECTS
 
| HID	| Status |	Names |	Project| 
|-|--|---|---------------|
| [147](https://github.com/cloudmesh-community/fa19-516-147/blob/master/project/report.md)	| approved	| Harsha	| Abstract database management on Multicloud environments for the NIST Big Data Reference Architecture AWS, Azure |
| [141](https://github.com/cloudmesh-community/fa19-516-141/blob/master/project/report.md)	| approved| 	Bala, Mani	| Abstract database management on Multicloud environments for the NIST Big Dara Refernce Architecture AWS, Azure, Mongo, SQL NIST BigDataInterface reference implementation for data base absractions on 2 clouds with 2 differnt technologies, one is a SQL based the othe is a NoSQPL based (mongo) |

## OTHER PROJECTS

| HID	| Status |	Names |	Project| 
|-|--|---|---------------|
| [167](https://github.com/cloudmesh-community/fa19-516-167/blob/master/project/report.md) | approved |Bill	|Cloudmesh Virtual Directory Life Cycle Service, This needs to run on 2 clouds + locally.  TTL = time to live is one, attribute for lifecycle management. Bi9ll explained this is more than ttl. |
| [166](https://github.com/cloudmesh-community/fa19-516-166/blob/master/project/report.md) |	approved |Brian Funk	| Cloudmesh Frugal with AWS, Azure, and Google Cloud. |
| [159](https://github.com/cloudmesh-community/fa19-516-159/blob/master/project/report.md) | TBD | Austin Zebrowski | Apache Airflow data pipelining between HDFS and Amazon Redshift |
| [171](https://github.com/cloudmesh-community/fa19-516-171/blob/master/project/report.md) | TBD	| Jagadesh	| Title missing |
| [144](https://github.com/cloudmesh-community/fa19-516-144/blob/master/project/report.md) [172](https://github.com/cloudmesh-community/fa19-516-172/blob/master/project/report.md) | TBD	| Andrew Holland, Nayeem | Introduce Encryption Functionality for Cloudmesh Configuration File |
| [161](https://github.com/cloudmesh-community/fa19-516-161/blob/master/project/report.md) | approved |	Jim Nelson	| Batch Processing using Kubernetes and Cloudmesh batch processing - of r programs - virtual cluster for batch processing r programs in kubernetes / vms cloudmesh may help. gregor suggests to use kubernetes to do batch jobs on it. Possibly cloudmeh-flow would help, but we do the jobs on cubernetes, while the bactch jobs are formulated as workflows.  |
| [153](https://github.com/cloudmesh-community/fa19-516-153/blob/master/project/report.md) [164](https://github.com/cloudmesh-community/fa19-516-164/blob/master/project/report.md)	| approved |	Anish, Siddhesh	| Hadoop/Spark Cluster abstraction layer - dynamically changing available machines/storage available to hadoop/spark instances using orchestration tool and command line  |
| [174](https://github.com/cloudmesh-community/fa19-516-174/blob/master/project/report.md) | TBD |		Sahithi	R |  |
| [151](https://github.com/cloudmesh-community/fa19-516-151/blob/master/project/report.md) | approved | Qiwei Liu, Yanting, Chenxu	| Cloudmesh Cloud AI Services.  |
| [140](https://github.com/cloudmesh-community/fa19-516-140/blob/master/project/report.md) | approved but some details are missing| Mohamed Elfateh	| Cognitive Response Simulation using Cloudmesh Cloud AI Services |
| [146](https://github.com/cloudmesh-community/fa19-516-146/blob/master/project/report.md) [163](https://github.com/cloudmesh-community/fa19-516-163/blob/master/project/report.md) | TBD | John Hoerr, Kenneth Jones| Serverless API Performance Analysis |
| [168](https://github.com/cloudmesh-community/fa19-516-168/blob/master/project/report.md)	| 	Deepak Deopura | TBD| 	AWS to/from Azure data transfer using APIs. -Extended version can be push data in SQL base warehouse (for example snowflake warehouse. It may be out of scope for now for this project purpose. |
