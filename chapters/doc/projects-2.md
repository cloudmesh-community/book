# Projects {#c:projects}

Class: E516 {#s:e516-project}
-----------

### Scope of Project

The objective of the project is to define a clear problem statement and
create a framework to address that problem. This framework must include
a docker packaged service that includes all necessary dependencies
necessary to perform the analysis. For example if you are using a
machine learning algorithm your docker packaged service will include the
machine learning algorithm which works on a particular dataset providing
clustered ,classified or regression outputs to the user. The final
objective is to obtain results in form of graphs or tabular mode. In
selecting a dataset, you can use a dataset from a public dataset like
and pick a suitable dataset according to your problem statement. Final
results must be exposed via a REST service. For instance if it is a
classification problem, the should have the capability of entering a new
test data set or single data point (meaning a single record) via a REST
API endpoint and get the expected output in form of a JSON object. UI
creation is an optional task. This is the overall expectation of the
project.

### Deliverables

-   Finding a Data Set Size(A good judgement of the datasize would be
    100 MB \< datasize \< 500MB): (Estimated Time 1 hour)

-   Cleaned Up Data Set: (Estimated Time 0.5 Week)

-   Spark Mllib or Scikit Learn ML Algorithm Application: (Estimated
    Time 0.5 Week) (You can use any other library you prefer)

-   Clustering or Regression or Classification results in Graphical
    Format (Matplotlib) (GUI optional, save graphs as png). Otherwise
    you can do some analysis on the dataset the way you propose to do
    and this way must be explicitly discussed with Professor Gregor. :
    (Estimated Time 5 days)

-   Swagger or Flask Rest Service to send a sample data set and get
    output (terminal output is enough): (Estimated Time 1 day)
    (Completing with Swagger Bonus points)

-   Take results in a constant environment (ex: Chameleon Cloud or your
    PC) provide system configuration: (Estimated Time 2 days)

-   Create a Makefile with running, setting up, sample test case running
    commands: (Estimated Time 1 hour)

-   requirement.txt file for pip installation: (Estimated Time \< 1
    hour)

-   Package with Docker: (Estimated Time 10 hour)

-   8 Page Report including (Abstract, Introduction, Project Procedure,
    Technology Usage, Results(Training Time breakdown, Data cleaning
    time breakdown, system set up time, time taken to system launch,
    define the performance of the machine you're taking results),
    Conclusion ,Work Distribution (if you have team members): (40 hours)

-   Dockerfile to run your project. (Your project must run in the docker
    environment, working project within docker is required).

Total Estimated Time : 3 weeks.

#### Grading Scheme

-   Working Project Inside Docker + Dockerfile : 60%

-   Project Report : 40%

### Helpful Chapters

You can learn how to use docker, referring to section
[\[S:docker-local\]](#S:docker-local){reference-type="ref"
reference="S:docker-local"}. Using docker in Chameleon clouds can be
learn via section [\[S:docker-fg\]](#S:docker-fg){reference-type="ref"
reference="S:docker-fg"}. Need more clarification regarding steps in
data cleaning and data processing, refer the section
[\[s:e222-proj-exp\]](#s:e222-proj-exp){reference-type="ref"
reference="s:e222-proj-exp"}.

Class: E616 {#s:e616-project}
-----------

### Scope of Project

The objective of the project is to define a clear problem statement and
create a framework to address that problem. This framework must include
a docker packaged service that includes all necessary dependencies
necessary to perform the analysis. For example if you are using a
machine learning algorithm your docker packaged service will include the
machine learning algorithm which works on a particular dataset providing
clustered ,classified or regression outputs to the user. The final
objective is to obtain results in form of graphs or tabular mode. In
selecting a dataset, you can use a dataset from a public dataset like
and pick a suitable dataset according to your problem statement. Final
results must be exposed via a REST service. For instance if it is a
classification problem, the should have the capability of entering a new
test data set or single data point (meaning a single record) via a REST
API endpoint and get the expected output in form of a JSON object. UI
creation is an optional task. This is the overall expectation of the
project.

### Deliverables

-   Finding a Data Set Size(A good judgement of the datasize would be
    100 MB \< datasize \< 500MB): (Estimated Time 1 hour)

-   Cleaned Up Data Set: (Estimated Time 0.5 Week)

-   Spark Mllib or Scikit Learn ML Algorithm Application: (Estimated
    Time 0.5 Week) (You can use any other library you prefer)

-   Clustering or Regression or Classification results in Graphical
    Format (Matplotlib) (GUI optional, save graphs as png). Otherwise
    you can do some analysis on the dataset the way you propose to do
    and this way must be explicitly discussed with Professor Gregor. :
    (Estimated Time 5 days)

-   Swagger or Flask Rest Service to send a sample data set and get
    output (terminal output is enough): (Estimated Time 1 day)
    (Completing with Swagger Bonus points)

-   Take results in a constant environment (ex: Chameleon Cloud or your
    PC) provide system configuration: (Estimated Time 2 days)

-   Create a Makefile with running, setting up, sample test case running
    commands: (Estimated Time 1 hour)

-   requirement.txt file for pip installation: (Estimated Time \< 1
    hour)

-   Package with Docker: (Estimated Time 10 hour)

-   8 Page Report including (Abstract, Introduction, Project Procedure,
    Technology Usage, Results(Training Time breakdown, Data cleaning
    time breakdown, system set up time, time taken to system launch,
    define the performance of the machine you're taking results),
    Conclusion ,Work Distribution (if you have team members): (40 hours)

-   Dockerfile to run your project. (Your project must run in the docker
    environment, working project within docker is required).

Total Estimated Time : 3 weeks.

### Helpful Chapters

You can learn how to use docker, referring to section
[\[S:docker-local\]](#S:docker-local){reference-type="ref"
reference="S:docker-local"}. Using docker in Chameleon clouds can be
learn via section [\[S:docker-fg\]](#S:docker-fg){reference-type="ref"
reference="S:docker-fg"}. Need more clarification regarding steps in
data cleaning and data processing, refer the section
[\[s:e222-proj-exp\]](#s:e222-proj-exp){reference-type="ref"
reference="s:e222-proj-exp"}.

#### Grading Scheme

-   Working Project Inside Docker + Dockerfile : 60%

-   Project Report : 40%

Class: i524 {#s:i524-project}
-----------

### Scope of Project

The objective of the project is to define a clear problem statement and
create a framework to address that problem. This framework must include
a docker packaged service that includes all necessary dependencies
necessary to perform the analysis. For example if you are using a
machine learning algorithm your docker packaged service will include the
machine learning algorithm which works on a particular dataset providing
clustered ,classified or regression outputs to the user. The final
objective is to obtain results in form of graphs or tabular mode. In
selecting a dataset, you can use a dataset from a public dataset like
and pick a suitable dataset according to your problem statement. Final
results must be exposed via a REST service. For instance if it is a
classification problem, the should have the capability of entering a new
test data set or single data point (meaning a single record) via a REST
API endpoint and get the expected output in form of a JSON object. UI
creation is an optional task. This is the overall expectation of the
project.

### Deliverables

-   Finding a Data Set Size(A good judgement of the datasize would be
    100 MB \< datasize \< 500MB): (Estimated Time 1 hour)

-   Cleaned Up Data Set: (Estimated Time 0.5 Week)

-   Spark Mllib or Scikit Learn ML Algorithm Application: (Estimated
    Time 0.5 Week) (You can use any other library you prefer)

-   Clustering or Regression or Classification results in Graphical
    Format (Matplotlib) (GUI optional, save graphs as png). Otherwise
    you can do some analysis on the dataset the way you propose to do
    and this way must be explicitly discussed with Professor Gregor. :
    (Estimated Time 5 days)

-   Swagger or Flask Rest Service to send a sample data set and get
    output (terminal output is enough): (Estimated Time 1 day)
    (Completing with Swagger Bonus points)

-   Take results in a constant environment (ex: Chameleon Cloud or your
    PC) provide system configuration: (Estimated Time 2 days)

-   Create a Makefile with running, setting up, sample test case running
    commands: (Estimated Time 1 hour)

-   requirement.txt file for pip installation: (Estimated Time \< 1
    hour)

-   Package with Docker: (Estimated Time 10 hour)

-   8 Page Report including (Abstract, Introduction, Project Procedure,
    Technology Usage, Results(Training Time breakdown, Data cleaning
    time breakdown, system set up time, time taken to system launch,
    define the performance of the machine you're taking results),
    Conclusion ,Work Distribution (if you have team members): (40 hours)

-   Dockerfile to run your project. (Your project must run in the docker
    environment, working project within docker is required).

Total Estimated Time : 3 weeks.

### Helpful Chapters

You can learn how to use docker, referring to section
[\[S:docker-local\]](#S:docker-local){reference-type="ref"
reference="S:docker-local"}. Using docker in Chameleon clouds can be
learn via section [\[S:docker-fg\]](#S:docker-fg){reference-type="ref"
reference="S:docker-fg"}. Need more clarification regarding steps in
data cleaning and data processing, refer the section
[\[s:e222-proj-exp\]](#s:e222-proj-exp){reference-type="ref"
reference="s:e222-proj-exp"}.

#### Grading Scheme

-   Working Project Inside Docker + Dockerfile : 60%

-   Project Report : 40%

Class: E222 {#s:classe22-proj}
-----------

For the final project in this class you need to do the following.

-   Find a dataset : A sample data repository can be found at the link
    below however you are free to use data from elsewhere, like scikit
    learn libraries.

-   Use Scikit Learn Library to run Machine Learning algorithm depending
    on your problem.

-   Do some classifications, clustering or a regression calculation
    using a machine learning algorithm.

-   Use the results from classification to draw charts. You can use
    matplotlib to do this.

-   Use REST api to tun ML algorithm i.e. in k-means the user should be
    able to change the cluster number (k) through a RESTful service.

-   Use Flask Rest API to expose the data to the viewers. So people can
    send a data set and get the outputs as a json object.

### Deliverables

-   Find and clean up data set

-   Scikit Learn ML Algorithm Application to cleaned up data set

-   Clustering or Regression or Classification results in Graphical
    Format (Matplotlib) (GUI not needed, save graphs as png)

-   Flask Rest service to tune ML algorithm

-   Flask Rest Service to send a sample data set and get output
    (terminal output is enough)

-   Create a Makefile with running, setting up, sample test case running
    commands

-   requirement.txt file for pip installation

-   Package with Docker

-   Report including (Abstract, Introduction, Project Procedure,
    Technology Usage, Results, Work Distribution (if you have team
    members)

The remainder of this semester will be used to complete this project.

#### Grading Scheme

-   Working Project Inside Docker + Dockerfile : 60%

-   Project Report : 40%
