\FILENAME\
\section{Hadoop Virtual Cluster
Installation}\label{hadoop-virtual-cluster-installation}

\subsection{Cloudmesh Cluster
Installation}\label{cloudmesh-cluster-installation}

Before you start this lesson, you MUST finish cm\_install\_.

This lesson is created and test under the newest version of Cloudmesh
client. Update yours if not.

To manage virtual cluster on cloud, the command is \texttt{cm\ cluster}.
Try \texttt{cm\ cluster\ help} to see what other commands are and what
options they supported.

\subsubsection{Create Cluster}\label{create-cluster}

To create a virtual cluster on cloud, we must define an active cluster
specification with \texttt{cm\ cluster\ define} command. For example, we
define a cluster with 3 nodes:

\begin{verbatim}
$ cm cluster define --count 3
\end{verbatim}

\begin{description}
\item[All options will use the default setting if not specified during
cluster]
define. Try \texttt{cm\ cluster\ help} command to see what options
\texttt{cm\ cluster\ define} has and means, here is part of the usage
information: :
\end{description}

\$ cm cluster help usage: cluster create {[}-n NAME{]} {[}-c COUNT{]}
{[}-C CLOUD{]} {[}-u NAME{]} {[}-i IMAGE{]} {[}-f FLAVOR{]} {[}-k KEY{]}
{[}-s NAME{]} {[}-AI{]} Options: -A --no-activate Do not activate this
cluster -I --no-floating-ip Do not assign floating IPs -n NAME
--name=NAME Name of the cluster -c COUNT --count=COUNT Number of nodes
in the cluster -C NAME --cloud=NAME Name of the cloud -u NAME
--username=NAME Name of the image login user -i NAME --image=NAME Name
of the image -f NAME --flavor=NAME Name of the flavor -k NAME --key=NAME
Name of the key -s NAME --secgroup=NAME NAME of the security group -o
PATH --path=PATH Output to this path \ldots{}

\begin{description}
\item[Floating IP is a valuable and limited resource on cloud.]
\texttt{cm\ cluster\ define} will assign floating IP to every node
within the cluster by default. Cluster creation will fail if the
floating IPs run out on cloud. When you run into error like this, use
option \texttt{-I} or \texttt{-\/-no-floating-ip} to avoid assigning
floating IPs during cluster creation:
\end{description}

\$ cm cluster define --count 3 --no-floating-ip

\begin{quote}
Then manually assign floating IP to one of the nodes. Use this node as a
logging node or head node to log in to all the other nodes.
\end{quote}

We can have multiple specifications defined at the same time. Every time
a new cluster specification is defined, the counter of the default
cluster name will increment. Hence, the default cluster name will be
\texttt{cluster-001}, \texttt{cluster-002}, \texttt{cluster-003} and so
on. Use \texttt{cm\ cluster\ avail} to check all the available cluster
specifications:

\begin{verbatim}
$ cm cluster avail
  cluster-001
    count                         : 3
    image                         : CC-Ubuntu14.04
    key                           : xl41
    flavor                        : m1.small
    secgroup                      : default
    assignFloatingIP              : True
    cloud                         : chameleon
> cluster-002
    count                         : 3
    image                         : CC-Ubuntu14.04
    key                           : xl41
    flavor                        : m1.small
    secgroup                      : default
    assignFloatingIP              : False
    cloud                         : chameleon
\end{verbatim}

With \texttt{cm\ cluster\ use\ {[}NAME{]}}, we are able to switch
between different specifications with given cluster name:

\begin{verbatim}
$ cm cluster use cluster-001
$ cm cluster avail
> cluster-001
    count                         : 3
    image                         : CC-Ubuntu14.04
    key                           : xl41
    flavor                        : m1.small
    secgroup                      : default
    assignFloatingIP              : True
    cloud                         : chameleon
  cluster-002
    count                         : 3
    image                         : CC-Ubuntu14.04
    key                           : xl41
    flavor                        : m1.small
    secgroup                      : default
    assignFloatingIP              : False
    cloud                         : chameleon
\end{verbatim}

This will activate specification \texttt{cluster-001} which assigns
floating IP during creation rather than the latest one
\texttt{cluster-002}.

With our cluster specification ready, we create the cluster with command
\texttt{cm\ cluster\ allocate}. This will create a virtual cluster on
the cloud with the activated specification:

\begin{verbatim}
$ cm cluster allocate
\end{verbatim}

Each specification can have one active cluster, which means
\texttt{cm\ cluster\ \ \ allocate} does nothing if there is a
successfully active cluster.

\subsubsection{Check Created Cluster}\label{check-created-cluster}

With command \texttt{cm\ cluster\ list}, we can see the cluster with the
default name \texttt{cluster-001} we just created:

\begin{verbatim}
$ cm cluster list
cluster-001
\end{verbatim}

Using \texttt{cm\ cluster\ nodes\ {[}NAME{]}}, we can also see the nodes
of the cluster along with their assigned floating IPs of the cluster:

\begin{verbatim}
$ cm cluster nodes cluster-001
xl41-001 129.114.33.147
xl41-002 129.114.33.148
xl41-003 129.114.33.149
\end{verbatim}

If option \texttt{-\/-no-floating-ip} is included during definition, you
will see nodes without floating IP:

\begin{verbatim}
$ cm cluster nodes cluster-002
xl41-004 None
xl41-005 None
xl41-006 None
\end{verbatim}

To log in one of them, use command
\texttt{cm\ vm\ assign\ IP\ {[}NAME{]}} to assign a floating IP to one
of them:

\begin{verbatim}
$ cm vm ip assign xl41-006
$ cm cluster nodes cluster-002
xl41-004 None
xl41-005 None
xl41-006 129.114.33.150
\end{verbatim}

Then you can log in this node as a head node of your cluster by
\texttt{cm\ vm\ ssh\ {[}NAME{]}}:

\begin{verbatim}
$ cm vm ssh xl41-006
cc@xl41-006 $
\end{verbatim}

\subsubsection{Delete Cluster}\label{delete-cluster}

Using \texttt{cm\ cluster\ delete\ {[}NAME{]}}, we are able to delete
the cluster we created:

\begin{verbatim}
$ cm cluster delete cluster-001
\end{verbatim}

\begin{description}
\item[Option \texttt{-\/-all} can delete all the clusters created, so be
careful:]
:
\end{description}

\$ cm cluster delete --all

Then we need to undefine our cluster specification with command
\texttt{cm\ cluster\ undefine\ {[}NAME{]}}:

\begin{verbatim}
$ cm cluster undefine cluster-001
\end{verbatim}

Option \texttt{-\/-all} can delete all the cluster specifications:

\begin{verbatim}
$ cm cluster undefine --all
\end{verbatim}

\subsection{Hadoop Cluster
Installation}\label{hadoop-cluster-installation}

This section is built upon the previous one. Please finish the previous
one before start this one.

\subsubsection{Create Hadoop Cluster}\label{create-hadoop-cluster}

To create a Hadoop cluster, we need to first define a cluster with
\texttt{cm\ cluster\ define} command:

\begin{verbatim}
$ cm cluster define --count 3
\end{verbatim}

\begin{description}
\item[To deploy a Hadoop cluster, we only support image
\texttt{CC-Ubuntu14.04}]
on Chameleon. DO NOT use \texttt{CC-Ubuntu16.04} or any other images.
You will need to specify it if it's not the default image:
\end{description}

\$ cm cluster define --count 3 --image CC-Ubuntu14.04

Then we define the Hadoop cluster upon the cluster we defined using
\texttt{cm\ hadoop\ define} command:

\begin{verbatim}
$ cm hadoop define
\end{verbatim}

Same as \texttt{cm\ cluster\ define}, you can define multiple
specifications for the Hadoop cluster and check them with
\texttt{cm\ hadoop\ avail}:

\begin{verbatim}
$ cm hadoop avail
> stack-001
  local_path                    : /Users/tony/.cloudmesh/stacks/stack-001
  addons                        : []
\end{verbatim}

We can use \texttt{cm\ hadoop\ use\ {[}NAME{]}} to activate the
specification with the given name:

\begin{verbatim}
$ cm hadoop use stack-001
\end{verbatim}

May not be available for current version of Cloudmesh Client.

Before deploy, we need to use \texttt{cm\ hadoop\ sync} to checkout /
synchronize the Big Data Stack from Github.com:

\begin{verbatim}
$ cm hadoop sync
\end{verbatim}

\begin{description}
\item[To avoid errors, make sure you are able to connect to Github.com
using SSH:]
\url{https://help.github.com/articles/connecting-to-github-with-ssh/}.
\end{description}

Finally, we are ready to deploy our Hadoop cluster:

\begin{verbatim}
$ cm hadoop deploy
\end{verbatim}

This process could take up to 10 minutes based on your network.

To check Hadoop is working or not. Use \texttt{cm\ vm\ ssh} to log into
the \texttt{Namenode} of the Hadoop cluster. It's usually the first node
of the cluster:

\begin{verbatim}
$ cm vm ssh node-001
cc@hostname$
\end{verbatim}

Switch to user \texttt{hadoop} and check HDFS is set up or not:

\begin{verbatim}
cc@hostname$ sudo su - hadoop
hadoop@hostname$ hdfs dfs -ls /
Found 1 items
drwxrwx---   - hadoop hadoop,hadoopadmin          0 2017-02-15 17:26 /tmp
\end{verbatim}

Now the Hadoop cluster is properly installed and configured.

\subsubsection{Delete Hadoop Cluster}\label{delete-hadoop-cluster}

To delete the Hadoop cluster we created, use command
\texttt{cm\ cluster\ delete\ {[}NAME{]}} to delete the cluster with
given name:

\begin{verbatim}
$ cm cluster delete cluster-001
\end{verbatim}

Then undefine the Hadoop specification and the cluster specification:

\begin{verbatim}
$ cm hadoop undefine stack-001
$ cm cluster undefine cluster-001
\end{verbatim}

May not be available for current version of Cloudmesh Client.

\subsection{Advanced Topics with
Hadoop}\label{advanced-topics-with-hadoop}

\subsubsection{Hadoop Virtual Cluster with Spark and/or
Pig}\label{hadoop-virtual-cluster-with-spark-andor-pig}

To install Spark and/or Pig with Hadoop cluster, we first use command
\texttt{cm\ hadoop\ define} but with \texttt{ADDON} to define the
cluster specification.

For example, we create a 3-node Spark cluster with Pig. To do that, all
we need is to specify \texttt{spark} as an \texttt{ADDON} during Hadoop
definition:

\begin{verbatim}
$ cm cluster define --count 3
$ cm hadoop define spark pig
\end{verbatim}

Using \texttt{cm\ hadoop\ addons}, we are able to check the current
supported addon:

\begin{verbatim}
$ cm hadoop addons
\end{verbatim}

With \texttt{cm\ hadoop\ avail}, we can see the detail of the
specification for the Hadoop cluster:

\begin{verbatim}
$ cm hadoop avail
> stack-001
  local_path                    : /Users/tony/.cloudmesh/stacks/stack-001
  addons                        : [u'spark', u'pig']
\end{verbatim}

Then we use \texttt{cm\ hadoop\ sync} and \texttt{cm\ hadoop\ deploy} to
deploy our Spark cluster:

\begin{verbatim}
$ cm hadoop sync
$ cm hadoop deploy
\end{verbatim}

This process will take 15 minutes or longer.

Before we proceed to the next step, there is one more thing we need to,
which is to make sure we are able to ssh from every node to others
without password. To achieve that, we need to execute
\texttt{cm\ cluster\ cross\_ssh}:

\begin{verbatim}
$ cm cluster cross_ssh
\end{verbatim}

\subsubsection{Word Count Example on
Spark}\label{word-count-example-on-spark}

Now with the cluster ready, let's run a simple Spark job, Word Count, on
one of William Shakespeare's work. Use \texttt{cm\ vm\ ssh} to log into
the \texttt{Namenode} of the Spark cluster. It should be the first node
of the cluster:

\begin{verbatim}
$ cm vm ssh node-001
cc@hostname$
\end{verbatim}

Switch to user \texttt{hadoop} and check HDFS is set up or not:

\begin{verbatim}
cc@hostname$ sudo su - hadoop
hadoop@hostname$
\end{verbatim}

Download the input file from the Internet:

\begin{verbatim}
wget --no-check-certificate -O inputfile.txt \
https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt
\end{verbatim}

You can also use any other text file you preferred. Create a new
directory \texttt{wordcount} within HDFS to store the input and output:

\begin{verbatim}
$ hdfs dfs -mkdir /wordcount
\end{verbatim}

Store the input text file into the directory:

\begin{verbatim}
$ hdfs dfs -put inputfile.txt /wordcount/inputfile.txt
\end{verbatim}

Save the following code as \texttt{wordcount.py} on the local file
system on Namenode:

\begin{verbatim}
import sys

from pyspark import SparkContext, SparkConf

if __name__ == "__main__":

  # tak two arguments, input and output
  if len(sys.argv) != 3:
    print("Usage: wordcount <input> <output>")
    exit(-1)

  # create Spark context with Spark configuration
  conf = SparkConf().setAppName("Spark Count")
  sc = SparkContext(conf=conf)

  # read in text file
  text_file = sc.textFile(sys.argv[1])

  # split each line into words
  # count the occurrence of each word
  # sort the output based on word
  counts = text_file.flatMap(lambda line: line.split(" ")) \
           .map(lambda word: (word, 1)) \
           .reduceByKey(lambda a, b: a + b) \
           .sortByKey()

  # save the result in the output text file
  counts.saveAsTextFile(sys.argv[2])
\end{verbatim}

Next submit the job to Yarn and run in distribute:

\begin{verbatim}
$ spark-submit --master yarn --deploy-mode client --executor-memory 1g \
--name wordcount --conf "spark.app.id=wordcount" wordcount.py \
hdfs://192.168.0.236:8020/wordcount/inputfile.txt \
hdfs://192.168.0.236:8020/wordcount/output
\end{verbatim}

Finally, take a look at the result in the output directory:

\begin{verbatim}
$ hdfs dfs -ls /wordcount/outputfile/
Found 3 items
-rw-r--r--   1 hadoop hadoop,hadoopadmin          0 2017-03-07 21:28 /wordcount/output/_SUCCESS
-rw-r--r--   1 hadoop hadoop,hadoopadmin     483182 2017-03-07 21:28 /wordcount/output/part-00000
-rw-r--r--   1 hadoop hadoop,hadoopadmin     639649 2017-03-07 21:28 /wordcount/output/part-00001
$ hdfs dfs -cat /wordcount/output/part-00000 | less
(u'', 517065)
(u'"', 241)
(u'"\'Tis', 1)
(u'"A', 4)
(u'"AS-IS".', 1)
(u'"Air,"', 1)
(u'"Alas,', 1)
(u'"Amen"', 2)
(u'"Amen"?', 1)
(u'"Amen,"', 1)
...
\end{verbatim}
